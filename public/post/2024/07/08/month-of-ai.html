<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-06-19 Thu 00:11 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Month of AI</title>
<meta name="author" content="Will S. Medrano" />
<meta name="generator" content="Org Mode" />

 <link rel="stylesheet"
          href="https://fonts.googleapis.com/css?family=Quicksand">
<link rel="stylesheet" href="/css/htmlize-styles.css">
<link rel="stylesheet" href="/css/styles.css">
</head>
<body>

<div class="navbar" id="org-div-home-and-up">
 <!--Ignore -->
 <a accesskey="H" href="/">wmedrano dot dev</a>
</div><div id="content" class="content">
<h1 class="title">Month of AI</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org83f91a2">1. Introduction</a></li>
<li><a href="#orgff064d1">2. Background</a>
<ul>
<li><a href="#orgd1da022">2.1. Round 3: What I Did</a></li>
<li><a href="#orgc3bec69">2.2. Takeaways</a>
<ul>
<li><a href="#org6ede813">2.2.1. LLMs Can Run On Lots of Hardware</a></li>
<li><a href="#org2eb55b3">2.2.2. Running Models is Easyish</a></li>
<li><a href="#org651ebe4">2.2.3. Model Documentation is Trash</a></li>
<li><a href="#org343d793">2.2.4. Text Vectorization Alone is Crap</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-org83f91a2" class="outline-2">
<h2 id="org83f91a2"><span class="section-number-2">1.</span> Introduction</h2>
<div class="outline-text-2" id="text-1">
<p>
Generative AI models have been on fire lately. Proponents peddle it as
the next “internet” like revolution. Startups and big tech companies
can’t resist the urge to throw more money at it. Most of this has
given me FOMO over AI. Perhaps there is something I don’t
understand. In the latest attempt, I plugged in models from
Huggingface, which reminded me of a more modern but janky
<code>scikit-learn</code>.
</p>
</div>
</div>
<div id="outline-container-orgff064d1" class="outline-2">
<h2 id="orgff064d1"><span class="section-number-2">2.</span> Background</h2>
<div class="outline-text-2" id="text-2">
<p>
Round 1: In college (around 2015~2016) I messed around with some basic
machine learning. I implemented neural networks inference and training
in C++/Python/Julia. I plugged in several models from
<code>scikit-learn</code>. I bred feature-selectors using genetic algorithms. I
submitted mediocre models to Kaggle, an online machine learning
contest platform. I even skimmed the top placement’s creative
solutions. It was fun and all, but it was mostly the end of my machine
learning explorations.
</p>

<p>
Round 2: Over the years I didn’t use much machine learning. If
anything, I’ve been looking on how to reduce technological
distractions around me. When GitHub Copilot came out for technical
preview around Mid 2021, I signed up immediately. My expectations were
low, but I was impressed by the technology and curious to see how well
it would do. As expected, I was impressed with GitHub Copilot’s
technical achievement. As a tool though, I found the code generation
somewhat distracting (expected), but it was great and generating
docstrings on all my public <code>structs</code> and <code>functions</code>, an unexpected
surprise.
</p>
</div>
<div id="outline-container-orgd1da022" class="outline-3">
<h3 id="orgd1da022"><span class="section-number-3">2.1.</span> Round 3: What I Did</h3>
<div class="outline-text-3" id="text-2-1">
<p>
I delved into AI for a month or so.
</p>

<p>
I used Huggingface’s inference API to run some models. The inference
API (<a href="https://huggingface.co/docs/api-inference/en/detailed_parameters">documentation</a>) makes it fast and easy to run models, even on
potato hardware. However, not all model types are available using the
inference API, I especially wanted to try the text to vector
models. After reading the docs for downloading and running models
locally, I managed to slap together a half-baked search engine. The
final result of vector search was unimpressive.
</p>

<p>
The next stop was running LLMs. I <code>sudo pacman -S ollama</code> my way into
running the Phi-3 mini model. The model ran quickly and with a small
memory footprint due to the Q4 quantization cramming by cramming 16
bits information into just 4 bits. This breakthrough allowed me to
even run Phi-3 mini on a Raspberry Pi 5. The next step from this, I
ran <code>sudo pacman -S ollama-rocm</code> and the models ran faster on my
GPU. I really did not expect this to work at all. At the largest, I
was able to run a variant of the Codestral 22B model.
</p>
</div>
</div>
<div id="outline-container-orgc3bec69" class="outline-3">
<h3 id="orgc3bec69"><span class="section-number-3">2.2.</span> Takeaways</h3>
<div class="outline-text-3" id="text-2-2">
</div>
<div id="outline-container-org6ede813" class="outline-4">
<h4 id="org6ede813"><span class="section-number-4">2.2.1.</span> LLMs Can Run On Lots of Hardware</h4>
<div class="outline-text-4" id="text-2-2-1">
<p>
I purchased an AMD card last year expecting to do no ML work on
it. When models were able to run on the GPU, they ran pretty well. I
was able to run a Q4 quantized Llama 3 8B model as 80 tokens/second;
this is plenty fast.
</p>

<p>
I was also able to run Phi-3 mini on the Raspberry Pi 5. Although the
speeds were not acceptable for interactable use cases, the Pi 5 is
usable for some small scale offline processing.
</p>
</div>
</div>
<div id="outline-container-org2eb55b3" class="outline-4">
<h4 id="org2eb55b3"><span class="section-number-4">2.2.2.</span> Running Models is Easyish</h4>
<div class="outline-text-4" id="text-2-2-2">
<p>
Although some of the APIs are ugly, HuggingFace has succeeded at
democratizing models. It’s easy to find models and use them out of the
box. If even that’s too hard, plenty of companies provide an API
endpoint that offers ML as a service.
</p>
</div>
</div>
<div id="outline-container-org651ebe4" class="outline-4">
<h4 id="org651ebe4"><span class="section-number-4">2.2.3.</span> Model Documentation is Trash</h4>
<div class="outline-text-4" id="text-2-2-3">
<blockquote>
<p>
MumboJumbo uses FooBar and Baz to reach SOTA performance on FizzBuzz &gt;
eval and reaches a Big O(3.14) on Wumpus.
</p>
</blockquote>

<p>
Most HuggingFace model pages are terrible, especially if you aren’t
constantly following the ML space. At the very least, HuggingFace
forces a category on models, such as text-to-speech or
text-generation, but my wishlist for a card:
</p>

<ul class="org-ul">
<li>Performance characteristics</li>
<li>Model size</li>
<li>Demo</li>
<li>Brief description</li>
<li>Caveats</li>
<li>Special tokens (like <code>FIM_PREFIX</code>)</li>
</ul>
</div>
</div>
<div id="outline-container-org343d793" class="outline-4">
<h4 id="org343d793"><span class="section-number-4">2.2.4.</span> Text Vectorization Alone is Crap</h4>
<div class="outline-text-4" id="text-2-2-4">
<p>
Listen to any layman interview and you will hear someone jump to the
brilliant idea that ML must just produce the best search. In practice,
my search engine fell short with just sentence-to-vec. I got good
improvements by adding keyword search + classical natural language
processing techniques such as synonyms and stemming. A more insightful
discussion on vector embeddings for Search can be found in Lex
Fridman’s <a href="https://youtu.be/e-gwvmhyU7A?si=znZM8TD-9O28t0v2">interview</a> with Perplexity’s CEO.
</p>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">

<div></div>
<p class="postamble-title">Title: Month of AI</p>
<p class="author">Author: Will S. Medrano</p>
<p class="date">Date: 2024-07-08</p>
</div>
</body>
</html>
